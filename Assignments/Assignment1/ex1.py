import re, sys, random, math, collections

from collections import defaultdict


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a language model
        from a given text.
        It supports language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.model_dict = defaultdict(int)
        self.n_gram_dicts = {}
        self.n_gram_dicts_total_count = {}
        # a dictionary of the form {ngram:count}, holding counts of all ngrams in the specified text.
        self.chars = chars

    def build_model(self, text):
        """populates the instance variable model_dict.

            Args:
                text (str): the text to construct the model from.
        """
        if not self.chars:
            split_text = text.split()
            for n in range(1, self.n + 1):
                n_gram_dict, n_grams_total_count = self.generate_n_gram_dict(n, split_text)
                self.n_gram_dicts[f"{n}"] = n_gram_dict
                self.n_gram_dicts_total_count[f"{n}"] = n_grams_total_count
                if n == self.n:
                    self.model_dict = n_gram_dict
        else:
            for n in range(1, self.n + 1):
                n_gram_dict, n_grams_total_count = self.generate_n_gram_dict_char(n, text.lower())
                self.n_gram_dicts[f"{n}"] = n_gram_dict
                self.n_gram_dicts_total_count[f"{n}"] = n_grams_total_count
                if n == self.n:
                    self.model_dict = n_gram_dict

    def generate_n_gram_dict(self, n, split_text):
        """
        generates n_gram dictionary for words using the given n, based on the given normalized split text

            Args:
                n (int): he length of the markov unit
                split_text (str): normalized split text to construct the dict from.
            Return:
                dict. The generated dictionary.
                int. n-grams total count
        """
        n_gram_dict = defaultdict(int)
        n_grams_total_count = 0
        for start_index in range(len(split_text)):
            end_index = start_index + n
            if len(split_text) < end_index:  # if there are no more n-grams
                break
            else:
                n_gram = ' '.join(split_text[start_index:end_index])
                n_gram_dict[n_gram] = n_gram_dict[n_gram] + 1
            n_grams_total_count = n_grams_total_count + 1
        return n_gram_dict, n_grams_total_count

    def generate_n_gram_dict_char(self, n, text_lower):
        """
        generates n_gram dictionary for chars using the given n, based on the given normalized split text

            Args:
                n (int): he length of the markov unit
                text_lower (str): normalized (lowered) text to construct the dict from.
            Return:
                dict. The generated dictionary.
                int. n-grams total count
        """
        n_gram_dict = defaultdict(int)
        n_grams_total_count = 0
        for start_index in range(len(text_lower)):
            end_index = start_index + n
            if len(text_lower) < end_index:  # if there are no more n-grams
                break
            else:
                n_gram = text_lower[start_index:end_index]
                n_gram_dict[n_gram] = n_gram_dict[n_gram] + 1
            n_grams_total_count = n_grams_total_count + 1
        return n_gram_dict, n_grams_total_count

    def get_model_dictionary(self):
        """Returns the dictionary class object        """
        return self.model_dict

    def get_model_window_size(self):
        """Returning the size of the context window (the n in "n-gram")
        """
        return self.n

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted. If the length of the specified context exceeds (or equal to)
        the specified n, the method should return the a prefix of length n of the specified context.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        if not self.chars:
            if context is None:
                # # get the word with the highest frequency
                # context = max(self.n_gram_dicts["1"], key=self.n_gram_dicts["1"].get)
                # sampling initial context
                # uni_gram_probs_dict = {k: v/self.n_gram_dicts_total_count["1"] for k, v in self.n_gram_dicts["1"].items()}
                context = random.choices(list(self.n_gram_dicts["1"].keys()), weights=self.n_gram_dicts["1"].values(),
                                         k=1)[0]

            norm_context = normalize_text(context)
            split_context = norm_context.split()

            generated_text = split_context
            while len(generated_text) < n:  # while we can generate
                # advancing the context pointer
                current_context = generated_text[-self.n + 1:]
                # Finding candidates
                current_context_str = ' '.join(current_context)
                next_word_dict = self.n_gram_dicts[f"{len(current_context) + 1}"]
                candidates = {k: v for k, v in next_word_dict.items() if
                              current_context_str in k[:len(current_context_str)]}

                if len(candidates) == 0:
                    break
                else:
                    # # choose most likely following word token
                    # most_likely_next_word = max(candidates, key=candidates.get).split()[-1]
                    generated_text.append(
                        random.choices(list(candidates.keys()), weights=candidates.values(), k=1)[0].split()[-1])
            result = ' '.join(generated_text[:n])
        else:
            if context is None:
                # # get the word with the highest frequency
                # context = max(self.n_gram_dicts["1"], key=self.n_gram_dicts["1"].get)
                # sampling initial context
                # uni_gram_probs_dict = {k: v/self.n_gram_dicts_total_count["1"] for k, v in self.n_gram_dicts["1"].items()}
                context = random.choices(list(self.n_gram_dicts["1"].keys()), weights=self.n_gram_dicts["1"].values(),
                                         k=1)[0]

            lower_context = context.lower()

            generated_text = lower_context
            while len(generated_text) < n:  # while we can generate
                # advancing the context pointer
                current_context = generated_text[-self.n + 1:]
                # Finding candidates
                next_word_dict = self.n_gram_dicts[f"{len(current_context) + 1}"]
                candidates = {k: v for k, v in next_word_dict.items() if
                              current_context in k[:len(current_context)]}

                if len(candidates) == 0:
                    break
                else:
                    # # choose most likely following word token
                    # most_likely_next_word = max(candidates, key=candidates.get).split()[-1]
                    generated_text = generated_text + \
                                     random.choices(list(candidates.keys()), weights=candidates.values(), k=1)[
                                         0]
            result = generated_text
        return result

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        if not self.chars:

            split_text = normalize_text(text).split()

            n_grams = []
            start_index = 0
            is_smoothing_required = False
            for end_index in range(1, len(split_text) + 1):
                if end_index > len(split_text):
                    break
                if end_index - start_index > self.n:
                    start_index = start_index + 1
                n_gram_length = end_index - start_index
                n_gram = ' '.join(split_text[start_index:end_index])
                n_grams.append(n_gram)
                if n_gram not in self.n_gram_dicts[f"{n_gram_length}"]:
                    is_smoothing_required = True

            if is_smoothing_required:
                probs = [self.smooth(n_gram) for n_gram in n_grams]
            else:
                probs = [self.get_n_gram_probability(n_gram) for n_gram in n_grams]

            log_probs = [math.log(prob) for prob in probs]

            log_likelihood = sum(log_probs)
            # math.exp(log_likelihood)  # ?
        else:
            text_lower = text.lower()

            n_grams = []
            start_index = 0
            is_smoothing_required = False
            for end_index in range(1, len(text_lower) + 1):
                if end_index > len(text_lower):
                    break
                if end_index - start_index > self.n:
                    start_index = start_index + 1
                n_gram_length = end_index - start_index
                n_gram = text_lower[start_index:end_index]
                n_grams.append(n_gram)
                if n_gram not in self.n_gram_dicts[f"{n_gram_length}"]:
                    is_smoothing_required = True

            if is_smoothing_required:
                probs = [self.smooth(n_gram) for n_gram in n_grams]
            else:
                probs = [self.get_n_gram_probability(n_gram) for n_gram in n_grams]

            log_probs = [math.log(prob) for prob in probs]

            log_likelihood = sum(log_probs)
            # math.exp(log_likelihood)  # ?
        return log_likelihood

    def get_n_gram_probability(self, n_gram):
        """
        receives a normalized n gram and calculates its probability

            Args:
                n_gram (str): the ngram to have it's probability smoothed

            Returns:
                float. The ngram probability.
        """
        if not self.chars:
            split_n_gram = n_gram.split()
            count = self.n_gram_dicts[f"{len(split_n_gram)}"][n_gram]
            if len(split_n_gram) == 1:  # unigram
                total = self.n_gram_dicts_total_count[
                    f"{self.n}"]  # self.n_gram_dicts_total_count[f"{len(split_n_gram)}"]
            else:
                previous = split_n_gram[0:-1]
                total = self.n_gram_dicts[f"{len(previous)}"][' '.join(split_n_gram[0:-1])]
        else:
            count = self.n_gram_dicts[f"{len(n_gram)}"][n_gram]
            if len(n_gram) == 1:  # unigram
                total = self.n_gram_dicts_total_count[
                    f"{self.n}"]  # self.n_gram_dicts_total_count[f"{len(split_n_gram)}"]
            else:
                previous = n_gram[0:-1]
                total = self.n_gram_dicts[f"{len(previous)}"][n_gram[0:-1]]
        return count / total

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        if not self.chars:

            norm_ngram = normalize_text(ngram)
            split_ngram = norm_ngram.split()

            if norm_ngram in self.n_gram_dicts[f"{len(split_ngram)}"]:
                count = self.n_gram_dicts[f"{len(split_ngram)}"][norm_ngram]
            else:
                count = 0

            # Laplace
            count = count + 1

            if len(split_ngram) == 1:  # unigram
                total = self.n_gram_dicts_total_count[f"{self.n}"] + \
                        len(self.n_gram_dicts["1"])  # len(self.n_gram_dicts[f"{self.n}"])
            else:
                previous = split_ngram[0:-1]
                total = self.n_gram_dicts[f"{len(previous)}"][' '.join(split_ngram[0:-1])] + \
                        len(self.n_gram_dicts["1"])  # len(self.n_gram_dicts[f"{self.n}"])
        else:
            if ngram in self.n_gram_dicts[f"{len(ngram)}"]:
                count = self.n_gram_dicts[f"{len(ngram)}"][ngram]
            else:
                count = 0

            # Laplace
            count = count + 1

            if len(ngram) == 1:  # unigram
                total = self.n_gram_dicts_total_count[f"{self.n}"] + \
                        len(self.n_gram_dicts["1"])  # len(self.n_gram_dicts[f"{self.n}"])
            else:
                previous = ngram[0:-1]
                total = self.n_gram_dicts[f"{len(previous)}"][' '.join(ngram[0:-1])] + \
                        len(self.n_gram_dicts["1"])  # len(self.n_gram_dicts[f"{self.n}"])
        return count / total


def normalize_text(text):
    """Returns a normalized version of the specified string.
      You can add default parameters as you like (they should have default values!)
      You should explain your decisions in the header of the function.

      Args:
        text (str): the text to normalize

      Returns:
        string. the normalized text.
    """
    norm_text = text.lower()
    norm_text = re.sub('([.,!?()])', r' \1 ', norm_text)
    norm_text = re.sub('\s{2,}', ' ', norm_text)
    return norm_text


def who_am_i():  # this is not a class method
    """Returns a dictionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Yiftach Savransky', 'id': '312141369', 'email': 'yiftachs@post.bgu.ac.il'}
