{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Notebook Setup"
      ],
      "metadata": {
        "id": "PdtcXTdnepqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !wget -O trump_train.tsv https://uc86a09a9e080227baec2fc5c686.dl.dropboxusercontent.com/cd/0/get/Bb3v1RjwQxO_h06PrU1cHGzIpwuwSdWDh7WxSi7USXuVVjtR1Tu5BH3gGIeucXSdSGunuaS1Sng4kLSZm7NTNoahQtTnlUvv5EeZSe6Yzi3lJ4yy9D7M8aKw4RQRtDAcPUnULFUfpHNLW4D0O36drs4L/file?_download_id=88356818103661160667177451500551324168953997065185434572286524337&_notify_domain=www.dropbox.com&dl=1 \n",
        "drive_path = '/content/drive/MyDrive/NLP/Ex4/'\n",
        "train_file_path = drive_path+\"en-ud-train.upos.tsv\"\n",
        "test_file_path = drive_path+\"en-ud-dev.upos.tsv\"\n",
        "embeddings_file_path = drive_path+\"glove.6B.100d.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leXpFwR8PDfm",
        "outputId": "2b4d9f5a-4fd9-4401-ba0c-b954b6979780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "7-wx7k9gevDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nlp, assignment 4, 2021\n",
        "\n",
        "In this assignment you will implement a Hidden Markov model and an LSTM model\n",
        "to predict the part of speech sequence for a given sentence.\n",
        "(Adapted from Nathan Schneider)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext import data, vocab as torch_vocab\n",
        "\n",
        "from torchtext.legacy import data as data_l\n",
        "import torch.optim as optim\n",
        "from math import log, isfinite\n",
        "from collections import Counter\n",
        "\n",
        "import sys, os, time, platform, nltk, random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# With this line you don't need to worry about the HW  -- GPU or CPU\n",
        "# GPU cuda cores will be used if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# You can call use_seed with other seeds or None (for complete randomization)\n",
        "# but DO NOT change the default value.\n",
        "def use_seed(seed = 2512021):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.set_deterministic(True)\n",
        "    #torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# utility functions to read the corpus\n",
        "def who_am_i(): #this is not a class method\n",
        "    \"\"\"Returns a dictionary with your name, id number and email. keys=['name', 'id','email']\n",
        "        Make sure you return your own info!\n",
        "    \"\"\"\n",
        "    #edit the dictionary to have your own details\n",
        "    return {'name': 'Yiftach Savransky', 'id': '312141369', 'email': 'yiftachs@post.bgu.ac.il'}\n",
        "\n",
        "def read_annotated_sentence(f):\n",
        "    line = f.readline()\n",
        "    if not line:\n",
        "        return None\n",
        "    sentence = []\n",
        "    while line and (line != \"\\n\"):\n",
        "        line = line.strip()\n",
        "        word, tag = line.split(\"\\t\", 2)\n",
        "        sentence.append( (word, tag) )\n",
        "        line = f.readline()\n",
        "    return sentence\n",
        "\n",
        "def load_annotated_corpus(filename):\n",
        "    sentences = []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        sentence = read_annotated_sentence(f)\n",
        "        while sentence:\n",
        "            sentences.append(sentence)\n",
        "            sentence = read_annotated_sentence(f)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "dMxf_2tiHFCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learn params and Baseline Tagset"
      ],
      "metadata": {
        "id": "sCuCmoPJfCan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START = \"<DUMMY_START_TAG>\"\n",
        "END = \"<DUMMY_END_TAG>\"\n",
        "UNK = \"<UNKNOWN>\"\n",
        "\n",
        "allTagCounts = Counter()\n",
        "# use Counters inside these\n",
        "perWordTagCounts = {}\n",
        "transitionCounts = {}\n",
        "emissionCounts = {}\n",
        "# log probability distributions: do NOT use Counters inside these because\n",
        "# missing Counter entries default to 0, not log(0)\n",
        "A = {} #transisions probabilities\n",
        "B = {} #emmissions probabilities\n",
        "\n",
        "def get_all_tag_counts(annotated_corpus):\n",
        "  allTagCounts = Counter(annotated_token[1] for annotated_sentence in annotated_corpus for annotated_token in annotated_sentence)\n",
        "  # allTagCounts does not include pseudocounts, dummy tags and unknowns.\n",
        "  return allTagCounts\n",
        "\n",
        "def get_pairs_counts(annotated_corpus):\n",
        "  pairsCounts = Counter(annotated_token for annotated_sentence in annotated_corpus for annotated_token in annotated_sentence)\n",
        "  return pairsCounts\n",
        "\n",
        "def get_per_word_tag_counts(annotated_corpus):\n",
        "  pairsCounts = get_pairs_counts(annotated_corpus)\n",
        "  perWordTagdict = {}\n",
        "  perWordTagCounts = {}\n",
        "\n",
        "  for (token, tag), token_tag_count in pairsCounts.items():\n",
        "    if not token in perWordTagdict:\n",
        "      perWordTagdict[token] = {}\n",
        "    perWordTagdict[token][tag] = token_tag_count\n",
        "  for token_type, tags_dict in perWordTagdict.items():\n",
        "    perWordTagCounts[token_type] = Counter(tags_dict)\n",
        "  return Counter(perWordTagCounts)\n",
        "\n",
        "def get_emission_counts(annotated_corpus, all_tag_counts):\n",
        "  emissionCounts = {}\n",
        "  for tag in all_tag_counts:\n",
        "    emissionCounts[tag] = Counter(annotated_token[0] for annotated_sentence in annotated_corpus for annotated_token in annotated_sentence if annotated_token[1]==tag)\n",
        "  return Counter(emissionCounts)\n",
        "\n",
        "def get_transition_counts(annotated_corpus, all_tags):\n",
        "  #Call the new list a different name\n",
        "  corpus_tags = [[START]+[annotated_token[1] for annotated_token in annotated_sentence]+[END] for annotated_sentence in annotated_corpus]\n",
        "  #create all the pairs of tag_i, tag_i_+_1\n",
        "  corpus_tags_pairs = [[sentence_tags[tag_index],sentence_tags[tag_index+1] ] for sentence_tags in corpus_tags for tag_index in range(len(sentence_tags)-1)]\n",
        "\n",
        "  #counter(dict) of {tag_i: {tag_i_+_1:count}}  \n",
        "  transitionCounts = {}\n",
        "  for tag in all_tags:\n",
        "    transitionCounts[tag] = Counter(tags_pair[1] for tags_pair in corpus_tags_pairs if tags_pair[0]==tag)\n",
        "  return Counter(transitionCounts)\n",
        "\n",
        "def get_A(transitionCounts, tags):\n",
        "  A={}\n",
        "  for tag_i, next_tags_transition_count in transitionCounts.items():\n",
        "    A[tag_i] = {}\n",
        "    tag_i_count = max(sum(next_tags_transition_count.values()), 1) #for END token\n",
        "    tag_i_count += len(next_tags_transition_count) #for smoothing\n",
        "    \n",
        "    for tag_i_plus_1 in tags:\n",
        "      # with smoothing\n",
        "      count_tags_co_occurrence = 1\n",
        "      if tag_i_plus_1 in next_tags_transition_count:\n",
        "        count_tags_co_occurrence = count_tags_co_occurrence + next_tags_transition_count[tag_i_plus_1]\n",
        "\n",
        "      # A[tag_i][tag_i_plus_1] = count_tags_co_occurrence/tag_i_count\n",
        "      A[tag_i][tag_i_plus_1] = log(count_tags_co_occurrence/tag_i_count)\n",
        "  return A\n",
        "\n",
        "def get_B(emissionCounts, vocab):\n",
        "  B={}\n",
        "  for tag_i, next_token_emission_count in emissionCounts.items():\n",
        "    B[tag_i] = {}\n",
        "    tag_i_count = max(sum(next_token_emission_count.values()), 1) \n",
        "    tag_i_count += len(vocab) #for smoothing\n",
        "    \n",
        "    for next_token in vocab:\n",
        "      # with smoothing\n",
        "      count_tag_token_co_occurrence = 1\n",
        "      if next_token in next_token_emission_count:\n",
        "        count_tag_token_co_occurrence = count_tag_token_co_occurrence + next_token_emission_count[next_token]\n",
        "\n",
        "      B[tag_i][next_token] = log(count_tag_token_co_occurrence/tag_i_count)\n",
        "  return B"
      ],
      "metadata": {
        "id": "CME44jKzWX2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_params(tagged_sentences):\n",
        "  \"\"\"Populates and returns the allTagCounts, perWordTagCounts, transitionCounts,\n",
        "    and emissionCounts data-structures.\n",
        "    allTagCounts and perWordTagCounts should be used for baseline tagging and\n",
        "    should not include pseudocounts, dummy tags and unknowns.\n",
        "    The transisionCounts and emmisionCounts\n",
        "    should be computed with pseudo tags and shoud be smoothed.\n",
        "    A and B should be the log-probability of the normalized counts, based on\n",
        "    transisionCounts and emmisionCounts\n",
        "\n",
        "    Args:\n",
        "      tagged_sentences: a list of tagged sentences, each tagged sentence is a\n",
        "        list of pairs (w,t), as retunred by load_annotated_corpus().\n",
        "\n",
        "    Return:\n",
        "      [allTagCounts,perWordTagCounts,transitionCounts,emissionCounts,A,B] (a list)\n",
        "  \"\"\"\n",
        "  # START = \"<DUMMY_START_TAG>\"\n",
        "  # END = \"<DUMMY_END_TAG>\"\n",
        "  # UNK = \"<UNKNOWN>\"\n",
        "  vocab = {annotated_token[0] for annotated_sentence in tagged_sentences for annotated_token in annotated_sentence}\n",
        "\n",
        "  allTagCounts = get_all_tag_counts(tagged_sentences)\n",
        "  tags = list(allTagCounts.keys())+[START, END]\n",
        "\n",
        "  perWordTagCounts = get_per_word_tag_counts(tagged_sentences)\n",
        "  emissionCounts = get_emission_counts(tagged_sentences, allTagCounts)\n",
        "  transitionCounts = get_transition_counts(tagged_sentences, tags)\n",
        "  A = get_A(transitionCounts, tags)\n",
        "  B = get_B(emissionCounts, vocab)\n",
        "  return [allTagCounts,perWordTagCounts,transitionCounts,emissionCounts,A,B]\n",
        "\n",
        "def baseline_tag_sentence(sentence, perWordTagCounts, allTagCounts):\n",
        "  \"\"\"Returns a list of pairs (w,t) where each w corresponds to a word\n",
        "  (same index) in the input sentence. Each word is tagged by the tag most\n",
        "  frequently associated with it. OOV words are tagged by sampling from the\n",
        "  distribution of all tags.\n",
        "\n",
        "  Args:\n",
        "      sentence (list): a list of tokens (the sentence to tag)\n",
        "      perWordTagCounts (Counter): tags per word as specified in learn_params()\n",
        "      allTagCounts (Counter): tag counts, as specified in learn_params()\n",
        "\n",
        "      Return:\n",
        "      list: list of pairs\n",
        "  \"\"\"\n",
        "\n",
        "  tagged_sentence = []\n",
        "  for token in sentence:\n",
        "    if token in perWordTagCounts:\n",
        "      #Each word is tagged by the tag most frequently associated with it.\n",
        "      tag = max(perWordTagCounts[token], key=perWordTagCounts[token].get)\n",
        "    else:\n",
        "      #OOV words are tagged by sampling from the distribution of all tags.\n",
        "      tag = random.choices(list(allTagCounts.keys()), weights=list(allTagCounts.values()))[0]\n",
        "    tagged_sentence.append((token, tag))\n",
        "\n",
        "  return tagged_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "eXUh1PBSrSH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hidden Markov Model"
      ],
      "metadata": {
        "id": "8hsJuCeqfMGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================================\n",
        "#       POS tagging with HMM\n",
        "#===========================================\n",
        "\n",
        "def retrace_wrapper(end_item):\n",
        "    \"\"\"Wrapper for the recursive retrace function\n",
        "    \"\"\"\n",
        "    tags_sequence = retrace(end_item[1])\n",
        "    return tags_sequence\n",
        "def retrace(item):\n",
        "    \"\"\"Returns a list of tags (retracing the sequence with the highest probability,\n",
        "        reversing it and returning the list). The list correspond to the\n",
        "        list of words in the sentence (same indices).\n",
        "    \"\"\"\n",
        "    if item[1] is not None: #stopping criteria\n",
        "      prev_seq = retrace(item[1])\n",
        "      return prev_seq + [item[0]]\n",
        "    else:\n",
        "      return [item[0]] #return tag\n",
        "\n",
        "def hmm_tag_sentence(sentence, A, B):\n",
        "    \"\"\"Returns a list of pairs (w,t) where each w corresponds to a word\n",
        "    (same index) in the input sentence. Tagging is done with the Viterby\n",
        "    algorithm.\n",
        "\n",
        "    Args:\n",
        "        sentence (list): a list of tokens (the sentence to tag)\n",
        "        A (dict): The HMM Transition probabilities\n",
        "        B (dict): tthe HMM emmission probabilities.\n",
        "\n",
        "    Return:\n",
        "        list: list of pairs\n",
        "    \"\"\"\n",
        "    last_item = viterbi(sentence, A,B)\n",
        "    tags_sequence = retrace_wrapper(last_item)\n",
        "    tagged_sentence = [(sentence[i], tags_sequence[i]) for i in range(len(sentence))]\n",
        "\n",
        "    return tagged_sentence\n",
        "\n",
        "def viterbi(sentence, A,B):\n",
        "  \"\"\"Creates the Viterbi matrix, column by column. Each column is a list of\n",
        "  tuples representing cells. Each cell (\"item\") is a tupple (t,r,p), were\n",
        "  t is the tag being scored at the current position,\n",
        "  r is a reference to the corresponding best item from the previous position,\n",
        "  and p is a log probabilityof the sequence so far).\n",
        "\n",
        "  The function returns the END item, from which it is possible to\n",
        "  trace back to the beginning of the sentence.\n",
        "\n",
        "  Args:\n",
        "      sentence (list): a list of tokens (the sentence to tag)\n",
        "      A (dict): The HMM Transition probabilities\n",
        "      B (dict): tthe HMM emmission probabilities.\n",
        "\n",
        "  Return:\n",
        "      obj: the last item, tagged with END. should allow backtraking.\n",
        "\n",
        "      \"\"\"\n",
        "  # Hint 1: For efficiency reasons - for words seen in training there is no\n",
        "  #      need to consider all tags in the tagset, but only tags seen with that\n",
        "  #      word. For OOV you have to consider all tags.\n",
        "  # Hint 2: start with a dummy item  with the START tag (what would it log-prob be?).\n",
        "  #         current list = [ the dummy item ]\n",
        "  # Hint 3: end the sequence with a dummy: the highest-scoring item with the tag END\n",
        "\n",
        "  V = []\n",
        "\n",
        "  tags = list(B.keys())\n",
        "  #initialize\n",
        "  V.append([])\n",
        "  for next_tag in tags:\n",
        "    next_tag_log_prob = A[START][next_tag]\n",
        "    if sentence[0] in B[next_tag]:\n",
        "      p = next_tag_log_prob + B[next_tag][sentence[0]]\n",
        "    else: #OOV words get the min log-prob value of B[next_tag].values()\n",
        "      p = next_tag_log_prob + min(B[next_tag].values())\n",
        "    triplete = (next_tag, None, p)\n",
        "    V[0].append(triplete)\n",
        "\n",
        "\n",
        "  for token_index in range(1, len(sentence)):\n",
        "    V.append([])\n",
        "    token = sentence[token_index]\n",
        "    for curr_tag_index, curr_tag in enumerate(tags):\n",
        "      if token in B[curr_tag]:\n",
        "        B_curr_token = B[curr_tag][token]\n",
        "      else: #OOV words get the min log-prob value of B[next_tag].values()\n",
        "        B_curr_token = min(B[curr_tag].values())\n",
        "      \n",
        "      viterbi_scores = []\n",
        "      for previous_tag_index, previous_tag in enumerate(tags):\n",
        "        viterbi_previous_log_prob = V[token_index-1][previous_tag_index][2]\n",
        "        A_prev_curr = A[previous_tag][curr_tag]\n",
        "        viterbi_scores.append(viterbi_previous_log_prob+A_prev_curr+B_curr_token)\n",
        "\n",
        "      p = max(viterbi_scores)\n",
        "      prev_tag_index = np.argmax(viterbi_scores)\n",
        "      prev_tag_pointer = V[token_index-1][prev_tag_index]\n",
        "\n",
        "      triplete = (curr_tag, prev_tag_pointer, p)\n",
        "      V[token_index].append(triplete)\n",
        "\n",
        "  #Last item\n",
        "  last_item_index = np.argmax([x[2] for x in V[-1]])\n",
        "  last_item = (END, V[-1][last_item_index], V[-1][last_item_index][2])\n",
        "  return last_item\n",
        "\n",
        "\n",
        "#a suggestion for a helper function. Not an API requirement\n",
        "# def predict_next_best(word, tag, predecessor_list):\n",
        "# \"\"\"Returns a new item (tupple)\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "def joint_prob(sentence, A, B):\n",
        "  \"\"\"Returns the joint probability of the given sequence of words and tags under\n",
        "    the HMM model.\n",
        "\n",
        "    Args:\n",
        "        sentence (pair): a sequence of pairs (w,t) to compute.\n",
        "        A (dict): The HMM Transition probabilities\n",
        "        B (dict): the HMM emmission probabilities.\n",
        "    \"\"\"\n",
        "  p = 0   # joint log prob. of words and tags\n",
        "\n",
        "  first_token, first_tag = sentence[0]\n",
        "  first_tag_log_prob = A[START][first_tag]\n",
        "  if first_token in B[first_tag]:\n",
        "    p = first_tag_log_prob + B[first_tag][first_token]\n",
        "  else: #OOV words get the min log-prob value of B[next_tag].values()\n",
        "    p = first_tag_log_prob + min(B[first_tag].values())\n",
        "  \n",
        "  curr_tag = first_tag\n",
        "  for pair_index in range(1, len(sentence)):\n",
        "    previous_tag = curr_tag\n",
        "    curr_token, curr_tag = sentence[pair_index]\n",
        "    if curr_token in B[curr_tag]:\n",
        "      B_curr_token = B[curr_tag][curr_token]\n",
        "    else: #OOV words get the min log-prob value of B[next_tag].values()\n",
        "      B_curr_token = min(B[curr_tag].values())\n",
        "    A_prev_curr = A[previous_tag][curr_tag]\n",
        "    p = p + A_prev_curr + B_curr_token\n",
        "\n",
        "  assert isfinite(p) and p<0  # Should be negative. Think why!... because of the log...\n",
        "  return p\n",
        "\n"
      ],
      "metadata": {
        "id": "k9rFuwYnMHL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BiLSTM"
      ],
      "metadata": {
        "id": "KjXWOLGlzWP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================================\n",
        "#       POS tagging with BiLSTM\n",
        "#===========================================\n",
        "\n",
        "\"\"\" You are required to support two types of bi-LSTM:\n",
        "    1. a vanilla biLSTM in which the input layer is based on simple word embeddings\n",
        "    2. a case-based BiLSTM in which input vectors combine a 3-dim binary vector\n",
        "        encoding case information, see\n",
        "        https://arxiv.org/pdf/1510.06168.pdf\n",
        "\"\"\"\n",
        "\n",
        "# Suggestions and tips, not part of the required API\n",
        "#\n",
        "#  1. You can use PyTorch torch.nn module to define your LSTM, see:\n",
        "#     https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
        "#  2. You can have the BLSTM tagger model(s) implemented in a dedicated class\n",
        "#     (this could be a subclass of torch.nn.Module)\n",
        "#  3. Think about padding.\n",
        "#  4. Consider using dropout layers\n",
        "#  5. Think about the way you implement the input representation\n",
        "#  6. Consider using different unit types (LSTM, GRU,LeRU)\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, pad_idx):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            bidirectional=True,\n",
        "                            # batch_first=True, \n",
        "                            num_layers=num_layers)\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(int(hidden_dim*2), int(hidden_dim*2/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(int(hidden_dim*2/2), output_dim),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        out = self.linear_relu_stack(lstm_out)\n",
        "        return out\n",
        "\n",
        "class BiLSTMCaseModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, pad_idx):\n",
        "        super(BiLSTMCaseModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim+1, \n",
        "                            hidden_dim, \n",
        "                            bidirectional=True,\n",
        "                            # batch_first=True, \n",
        "                            num_layers=num_layers)\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(int(hidden_dim*2), int(hidden_dim*2/2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(int(hidden_dim*2/2), output_dim),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x_text, x_case):\n",
        "        embedded = self.embedding(x_text)\n",
        "\n",
        "        if len(x_case.size()) > 1:\n",
        "          x_case = x_case.reshape(x_case.size()[0], x_case.size()[1], 1)\n",
        "        else:\n",
        "          x_case = x_case.reshape(x_case.size()[0],1, 1)\n",
        "        \n",
        "        concat = torch.cat((embedded,x_case), 2)\n",
        "\n",
        "        lstm_out, _ = self.lstm(concat)\n",
        "\n",
        "        out = self.linear_relu_stack(lstm_out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "\n",
        "def load_pretrained_embeddings(path, vocab=None):\n",
        "  \"\"\" Returns an object with the the pretrained vectors, loaded from the\n",
        "      file at the specified path. The file format is the same as\n",
        "      https://www.kaggle.com/danielwillgeorge/glove6b100dtxt\n",
        "      You can also access the vectors at:\n",
        "        https://www.dropbox.com/s/qxak38ybjom696y/glove.6B.100d.txt?dl=0\n",
        "        (for efficiency (time and memory) - load only the vectors you need)\n",
        "      The format of the vectors object is not specified as it will be used\n",
        "      internaly in your code, so you can use the datastructure of your choice.\n",
        "\n",
        "  Args:\n",
        "      path (str): full path to the embeddings file\n",
        "      vocab (list): a list of words to have embeddings for. Defaults to None.\n",
        "\n",
        "  \"\"\"\n",
        "  #vectors = torchtext.vocab.Vectors(embeddings_file_path)\n",
        "  vocab_lower = [w.lower() for w in vocab]\n",
        "  vectors = {}\n",
        "  with open(path,'r') as f:\n",
        "      for line in f:\n",
        "          split_line = line.split()\n",
        "          word = split_line[0]\n",
        "          if word in vocab:\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            vectors[word] = embedding\n",
        "          elif word in vocab_lower:\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            vectors[word] = embedding\n",
        "  print(f\"{len(vectors)} words loaded!\")\n",
        "  return vectors\n",
        "\n",
        "\n",
        "def prepare_embeddings(embeddings_file_path, vocab):\n",
        "  \"\"\"\n",
        "  Loads the relevant embeddings and prepares them for the usage of the rnn model\n",
        "  \"\"\"\n",
        "  embeds = load_pretrained_embeddings(embeddings_file_path, vocab)\n",
        "\n",
        "  stoi = {}\n",
        "  embeds_vecs = []\n",
        "  for key, value in embeds.items():\n",
        "    i = len(stoi)\n",
        "    stoi[key] = i\n",
        "    embeds_vecs.append(torch.tensor(value))\n",
        "  return stoi, embeds_vecs\n",
        "\n",
        "def prepare_data(annotated_corpus, vocab, max_vocab_size, min_freq, batch_size, pretrained_embeddings_fn, embedding_dim, stoi=None, embeds_vecs=None):\n",
        "  \"\"\"\n",
        "  arrenges the training data for the training process\n",
        "  \"\"\"\n",
        "  train_tokens = [[annotated_pair[0] for annotated_pair in annotated_sentence] for annotated_sentence in annotated_corpus]\n",
        "  train_tags = [[annotated_pair[1] for annotated_pair in annotated_sentence] for annotated_sentence in annotated_corpus]\n",
        "  \n",
        "  src = ('text', data_l.Field(batch_first=False, lower=True)) \n",
        "  trg = ('labels', data_l.Field(is_target=True))\n",
        "\n",
        "  fields=[src,trg]\n",
        "  train_exp = [data_l.Example.fromlist(data=[train_tokens[i],train_tags[i]], fields=fields) for i in range(len(train_tokens))]\n",
        "\n",
        "  train_dataset = data_l.Dataset(examples = train_exp, fields=fields)\n",
        "\n",
        "  train_iter = data_l.BucketIterator(dataset=train_dataset, batch_size=batch_size, device=device, sort_key=lambda x: len(x.text))\n",
        "  # build the vocabulary\n",
        "  src[1].build_vocab(train_dataset, max_size=max_vocab_size, min_freq=min_freq)\n",
        "  trg[1].build_vocab(train_dataset)\n",
        "\n",
        "  if stoi is None or embeds_vecs is None:\n",
        "    stoi, embeds_vecs = prepare_embeddings(pretrained_embeddings_fn, vocab)\n",
        "  src[1].vocab.set_vectors(stoi=stoi, vectors=embeds_vecs, dim=embedding_dim)\n",
        "\n",
        "  return train_iter, src, trg, stoi, embeds_vecs\n",
        "\n",
        "def get_case_features(token):\n",
        "  # returns a three-dimensional binary vector to tell if wi is full lowercase, \n",
        "  # full uppercase or leading with a capital letter\n",
        "  # case_features = [0,0,0]\n",
        "  case_features = 0\n",
        "  if token.islower():\n",
        "    # case_features = [1,0,0]\n",
        "    case_features = 1\n",
        "  elif token.isupper():\n",
        "    # case_features = [0,1,0]\n",
        "    case_features = 2\n",
        "  elif token.istitle():\n",
        "    # case_features = [0,0,1]\n",
        "    case_features = 3\n",
        "  return case_features\n",
        "\n",
        "def prepare_data_case(annotated_corpus, vocab, max_vocab_size, min_freq, batch_size, pretrained_embeddings_fn, embedding_dim, stoi=None, embeds_vecs=None):\n",
        "  \"\"\"\n",
        "  arrenges the training data for the training process\n",
        "  \"\"\"\n",
        "  train_tokens = [[annotated_pair[0] for annotated_pair in annotated_sentence] for annotated_sentence in annotated_corpus]\n",
        "  train_tokens_case_feat = [[get_case_features(annotated_pair[0]) for annotated_pair in annotated_sentence] for annotated_sentence in annotated_corpus]\n",
        "  train_tags = [[annotated_pair[1] for annotated_pair in annotated_sentence] for annotated_sentence in annotated_corpus]\n",
        "  \n",
        "  src = ('text', data_l.Field(batch_first=False, lower=True)) \n",
        "  src_case = ('case', data_l.Field(use_vocab=False, dtype=torch.int, pad_token=0))\n",
        "  trg = ('labels', data_l.Field(is_target=True))\n",
        "\n",
        "  fields=[src,src_case,trg]\n",
        "  train_exp = [data_l.Example.fromlist(data=[train_tokens[i],train_tokens_case_feat[i],train_tags[i]], fields=fields) for i in range(len(train_tokens))]\n",
        "\n",
        "  train_dataset = data_l.Dataset(examples = train_exp, fields=fields)\n",
        "\n",
        "  train_iter = data_l.BucketIterator(dataset=train_dataset, batch_size=batch_size, device=device, sort_key=lambda x: len(x.text))\n",
        "  # build the vocabulary\n",
        "  src[1].build_vocab(train_dataset, max_size=max_vocab_size, min_freq=min_freq)\n",
        "  # src_case[1].build_vocab(train_dataset) \n",
        "  trg[1].build_vocab(train_dataset)\n",
        "\n",
        "  if stoi is None or embeds_vecs is None:\n",
        "    stoi, embeds_vecs = prepare_embeddings(pretrained_embeddings_fn, vocab)\n",
        "  src[1].vocab.set_vectors(stoi=stoi, vectors=embeds_vecs, dim=embedding_dim)\n",
        "\n",
        "  return train_iter, src, src_case, trg, stoi, embeds_vecs"
      ],
      "metadata": {
        "id": "P-Ywgu2ruYkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Init RNN"
      ],
      "metadata": {
        "id": "pyB3uX_kGOQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_rnn_model(params_d):\n",
        "  \"\"\"Returns a dictionary with the objects and parameters needed to run/train_rnn\n",
        "      the lstm model. The LSTM is initialized based on the specified parameters.\n",
        "      thr returned dict is may have other or additional fields.\n",
        "\n",
        "  Args:\n",
        "      params_d (dict): a dictionary of parameters specifying the model. The dict\n",
        "                      should include (at least) the following keys:\n",
        "                      {'max_vocab_size': max vocabulary size (int),\n",
        "                      'min_frequency': the occurence threshold to consider (int),\n",
        "                      'input_rep': 0 for the vanilla and 1 for the case-base (int),\n",
        "                      'embedding_dimension': embedding vectors size (int),\n",
        "                      'num_of_layers': number of layers (int),\n",
        "                      'output_dimension': number of tags in tagset (int),\n",
        "                      'pretrained_embeddings_fn': str,\n",
        "                      'data_fn': str\n",
        "                      }\n",
        "                      max_vocab_size sets a constraints on the vocab dimention.\n",
        "                          If the its value is smaller than the number of unique\n",
        "                          tokens in data_fn, the words to consider are the most\n",
        "                          frequent words. If max_vocab_size = -1, all words\n",
        "                          occuring more that min_frequency are considered.\n",
        "                      min_frequency privides a threshold under which words are\n",
        "                          not considered at all. (If min_frequency=1 all words\n",
        "                          up to max_vocab_size are considered;\n",
        "                          If min_frequency=3, we only consider words that appear\n",
        "                          at least three times.)\n",
        "                      input_rep (int): sets the input representation. Values:\n",
        "                          0 (vanilla), 1 (case-base);\n",
        "                          <other int>: other models, if you are playful\n",
        "                      The dictionary can include other keys, if you use them,\n",
        "                            BUT you shouldn't assume they will be specified by\n",
        "                            the user, so you should spacify default values.\n",
        "  Return:\n",
        "      a dictionary with the at least the following key-value pairs:\n",
        "                                      {'lstm': torch.nn.Module object,\n",
        "                                      input_rep: [0|1]}\n",
        "      #Hint: you may consider adding the embeddings and the vocabulary\n",
        "      #to the returned dict\n",
        "  \"\"\"\n",
        "  annotated_corpus=load_annotated_corpus(train_file_path)\n",
        "  vocab = {annotated_token[0] for annotated_sentence in annotated_corpus for annotated_token in annotated_sentence}\n",
        "  \n",
        "  batch_size=16\n",
        "\n",
        "  max_vocab_size = params_d[\"max_vocab_size\"] if params_d[\"max_vocab_size\"] > 0 else None\n",
        "\n",
        "  if params_d[\"input_rep\"] == 0:\n",
        "    train_iter, src, trg, stoi, embeds_vecs = prepare_data(annotated_corpus, vocab, max_vocab_size, params_d[\"min_frequency\"], batch_size=batch_size, pretrained_embeddings_fn=params_d[\"pretrained_embeddings_fn\"], embedding_dim=params_d[\"embedding_dimension\"])\n",
        "\n",
        "    # Model initialization\n",
        "    input_dim = len(src[1].vocab)\n",
        "    pad_idx = src[1].vocab.stoi[src[1].pad_token]\n",
        "    hidden_dim = 128\n",
        "    output_dim=params_d[\"output_dimension\"] if params_d[\"output_dimension\"]==len(trg[1].vocab.itos) else len(trg[1].vocab.itos)\n",
        "\n",
        "    lstm_model = BiLSTMModel(input_dim=input_dim,embedding_dim=params_d[\"embedding_dimension\"], hidden_dim=hidden_dim, num_layers=params_d[\"num_of_layers\"], output_dim=output_dim, pad_idx=pad_idx)\n",
        "  \n",
        "  elif params_d[\"input_rep\"] == 1:\n",
        "    train_iter, src, src_case, trg, stoi, embeds_vecs = prepare_data_case(annotated_corpus, vocab, max_vocab_size, params_d[\"min_frequency\"], batch_size=batch_size, pretrained_embeddings_fn=params_d[\"pretrained_embeddings_fn\"], embedding_dim=params_d[\"embedding_dimension\"])\n",
        "    # Model initialization\n",
        "    input_dim = len(src[1].vocab)\n",
        "    pad_idx = src[1].vocab.stoi[src[1].pad_token]\n",
        "    hidden_dim = 128\n",
        "    output_dim=params_d[\"output_dimension\"] if params_d[\"output_dimension\"]==len(trg[1].vocab.itos) else len(trg[1].vocab.itos)\n",
        "\n",
        "    lstm_model = BiLSTMCaseModel(input_dim=input_dim,embedding_dim=params_d[\"embedding_dimension\"], hidden_dim=hidden_dim, num_layers=params_d[\"num_of_layers\"], output_dim=output_dim, pad_idx=pad_idx)\n",
        "  \n",
        "\n",
        "  lstm_model.apply(init_weights)\n",
        "  pretrained_embeddings = src[1].vocab.vectors\n",
        "  lstm_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "  lstm_model.embedding.weight.data[pad_idx] = torch.zeros(params_d[\"embedding_dimension\"])\n",
        "  print(lstm_model)\n",
        "\n",
        "  model = {\n",
        "    'lstm': lstm_model,\n",
        "    'input_rep': params_d[\"input_rep\"],\n",
        "    'vocab': vocab,\n",
        "    'max_vocab_size': max_vocab_size,\n",
        "    'min_freq': params_d[\"min_frequency\"],\n",
        "    'pretrained_embeddings_fn': params_d[\"pretrained_embeddings_fn\"],\n",
        "    'stoi': stoi, \n",
        "    'embeds_vecs': embeds_vecs,\n",
        "    'embedding_dim': params_d[\"embedding_dimension\"],\n",
        "    'epochs': 80, \n",
        "    'learning_rate': 0.15,\n",
        "    'batch_size': batch_size\n",
        "  }\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_rnn(model, train_data, val_data = None):\n",
        "    \"\"\"Trains the BiLSTM model on the specified data.\n",
        "\n",
        "    Args:\n",
        "        model (dict): the model dict as returned by initialize_rnn_model()\n",
        "        train_data (list): a list of annotated sentences in the format returned\n",
        "                            by load_annotated_corpus()\n",
        "        val_data (list): a list of annotated sentences in the format returned\n",
        "                            by load_annotated_corpus() to be used for validation.\n",
        "                            Defaults to None\n",
        "        input_rep (int): sets the input representation. Defaults to 0 (vanilla),\n",
        "                         1: case-base; <other int>: other models, if you are playful\n",
        "    \"\"\"\n",
        "    #Tips:\n",
        "    # 1. you have to specify an optimizer\n",
        "    # 2. you have to specify the loss function and the stopping criteria\n",
        "    # 3. consider using batching\n",
        "    # 4. some of the above could be implemented in helper functions (not part of\n",
        "    #    the required API)\n",
        "    if model[\"input_rep\"] == 0:\n",
        "      train_iter, src, trg, stoi, embeds_vecs = prepare_data(train_data, model['vocab'], model['max_vocab_size'], model[\"min_freq\"], model[\"batch_size\"], pretrained_embeddings_fn=model[\"pretrained_embeddings_fn\"], embedding_dim=model[\"embedding_dim\"], stoi=model[\"stoi\"], embeds_vecs=model[\"embeds_vecs\"])\n",
        "      model[\"src\"] = src\n",
        "      model[\"trg\"] = trg\n",
        "    elif model[\"input_rep\"] == 1:\n",
        "      train_iter, src, src_case, trg, stoi, embeds_vecs = prepare_data_case(train_data, model['vocab'], model['max_vocab_size'], model[\"min_freq\"], model[\"batch_size\"], pretrained_embeddings_fn=model[\"pretrained_embeddings_fn\"], embedding_dim=model[\"embedding_dim\"], stoi=model[\"stoi\"], embeds_vecs=model[\"embeds_vecs\"])\n",
        "      model[\"src\"] = src\n",
        "      model[\"src_case\"] = src_case\n",
        "      model[\"trg\"] = trg\n",
        "\n",
        "\n",
        "    optimizer = optim.SGD(model[\"lstm\"].parameters(), lr=model['learning_rate'])\n",
        "    tag_pad_idx = trg[1].vocab.stoi[trg[1].pad_token]\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = tag_pad_idx)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10.0, gamma=0.8)\n",
        "\n",
        "    model[\"lstm\"] = model[\"lstm\"].to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    train(model=model[\"lstm\"], opt=optimizer, crit=criterion, scheduler=scheduler, train_iter=train_iter, epochs=model['epochs'], tag_pad_idx=tag_pad_idx, input_rep=model[\"input_rep\"])\n",
        "\n",
        "\n",
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / y[non_pad_elements].shape[0]\n",
        "\n",
        "def pad_case(case_vecs, pad=None):\n",
        "  if pad is None:\n",
        "    pad = 0\n",
        "  target_pad_length = max(len(vec) for vec in case_vecs)\n",
        "  padded = []\n",
        "  for vec in case_vecs:\n",
        "    while len(vec) < target_pad_length:\n",
        "      vec.append(pad)\n",
        "    padded.append(vec)\n",
        "  padded_tensor = torch.Tensor(padded)\n",
        "  padded_tensor = padded_tensor.reshape(padded_tensor.size()[1], padded_tensor.size()[0], padded_tensor.size()[2])\n",
        "  return padded_tensor\n",
        "\n",
        "def train_epoch(model, opt, crit, train_iter, tag_pad_idx, input_rep):\n",
        "    model.train(mode=True) #setting the model to training mode\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in train_iter:\n",
        "        if input_rep == 0:\n",
        "          predicted_label = model(batch.text)\n",
        "        elif input_rep == 1:\n",
        "          predicted_label = model(batch.text, batch.case)\n",
        "          \n",
        "        labels = batch.labels  \n",
        "        opt.zero_grad()\n",
        "\n",
        "        predicted_label = predicted_label.view(-1, predicted_label.shape[-1])\n",
        "        labels = labels.view(-1)\n",
        "\n",
        "        loss = crit(predicted_label, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        acc = categorical_accuracy(predicted_label, labels, tag_pad_idx)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(train_iter), epoch_acc / len(train_iter)\n",
        "\n",
        "def train(model, opt, crit, scheduler, train_iter, epochs, tag_pad_idx, input_rep):\n",
        "  total_accuracy = None\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      epoch_start_time = time.time()\n",
        "      train_loss, train_acc = train_epoch(model=model, opt=opt, crit=crit, train_iter=train_iter, tag_pad_idx=tag_pad_idx, input_rep=input_rep)\n",
        "      scheduler.step()\n",
        "      \n",
        "      print('-' * 59)\n",
        "      print('| end of epoch {:3d} | time: {:5.2f}s '.format(epoch,time.time() - epoch_start_time))\n",
        "                                            # 'valid accuracy {:8.3f} ',accuracy_val))\n",
        "      print(f\"\\|train accuracy {train_acc} \\| train loss {train_loss} \\| last_lr: {str(scheduler.get_last_lr())}\") #auc_val: {auc_val}, \n",
        "\n",
        "\n",
        "def rnn_tag_sentence(sentence, model):\n",
        "  \"\"\" Returns a list of pairs (w,t) where each w corresponds to a word\n",
        "      (same index) in the input sentence and t is the predicted tag.\n",
        "\n",
        "  Args:\n",
        "      sentence (list): a list of tokens (the sentence to tag)\n",
        "      model (dict):  a dictionary with the trained BiLSTM model and all that is needed\n",
        "                      to tag a sentence.\n",
        "\n",
        "  Return:\n",
        "      list: list of pairs\n",
        "  \"\"\"\n",
        "  model[\"lstm\"].eval()\n",
        "\n",
        "  if model[\"src\"][1].lower:\n",
        "        tokens = [t.lower() for t in sentence] \n",
        "\n",
        "  numericalized_tokens = [model[\"src\"][1].vocab.stoi[t] for t in tokens]\n",
        "\n",
        "  token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "  \n",
        "  token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "        \n",
        "  if model[\"input_rep\"] == 0:\n",
        "    predictions = model[\"lstm\"](token_tensor)\n",
        "  elif model[\"input_rep\"] == 1:\n",
        "    case_feat = torch.tensor([get_case_features(t) for t in sentence]).to(device)\n",
        "    predictions = model[\"lstm\"](token_tensor,case_feat)\n",
        "  \n",
        "  top_predictions = predictions.argmax(-1)\n",
        "  \n",
        "  predicted_tags = [model[\"trg\"][1].vocab.itos[t.item()] for t in top_predictions]\n",
        "  \n",
        "  tagged_sentence = [(sentence[i],predicted_tags[i]) for i in range(len(sentence))]\n",
        "  return tagged_sentence\n",
        "\n",
        "\n",
        "def get_best_performing_model_params():\n",
        "  \"\"\"Returns a disctionary specifying the parameters of your best performing\n",
        "      BiLSTM model.\n",
        "      IMPORTANT: this is a *hard coded* dictionary that will be used to create\n",
        "      a model and train a model by calling\n",
        "              initialize_rnn_model() and train_lstm()\n",
        "  \"\"\"\n",
        "  param_dict = {'max_vocab_size': -1,# max vocabulary size (int),\n",
        "    'min_frequency': 4,# the occurence threshold to consider (int),\n",
        "    'input_rep': 1,# 0 for the vanilla and 1 for the case-base (int),\n",
        "    'embedding_dimension': 100,# embedding vectors size (int),\n",
        "    'num_of_layers': 2,# number of layers (int),\n",
        "    'output_dimension': 19,# number of tags in tagset (int), !!!INCLUDE UNKOWN and Pad!!!\n",
        "    'pretrained_embeddings_fn': embeddings_file_path,# str, TODO: just the file name\n",
        "    'data_fn': train_file_path,# str TODO: just the file name\n",
        "  }\n",
        "  return param_dict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ogJXKKKuvvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===========================================================\n",
        "#       Wrapper function (tagging with a specified model)\n",
        "#===========================================================\n",
        "\n",
        "def tag_sentence(sentence, model):\n",
        "    \"\"\"Returns a list of pairs (w,t) where pair corresponds to a word (same index) in\n",
        "    the input sentence. Tagging is done with the specified model.\n",
        "\n",
        "    Args:\n",
        "        sentence (list): a list of tokens (the sentence to tag)\n",
        "        model (dict): a dictionary where key is the model name and the value is\n",
        "           an ordered list of the parameters of the trained model (baseline, HMM)\n",
        "           or the model isteld and the input_rep flag (LSTMs).\n",
        "\n",
        "        Models that must be supported (you can add more):\n",
        "        1. baseline: {'baseline': [perWordTagCounts, allTagCounts]}\n",
        "        2. HMM: {'hmm': [A,B]}\n",
        "        3. Vanilla BiLSTM: {'blstm':[model_dict]}\n",
        "        4. BiLSTM+case: {'cblstm': [model_dict]}\n",
        "        5. (NOT REQUIRED: you can add other variations, agumenting the input\n",
        "            with further subword information, with character-level word embedding etc.)\n",
        "\n",
        "        The parameters for the baseline model are:\n",
        "        perWordTagCounts (Counter): tags per word as specified in learn_params()\n",
        "        allTagCounts (Counter): tag counts, as specified in learn_params()\n",
        "\n",
        "        The parameters for the HMM are:\n",
        "        A (dict): The HMM Transition probabilities\n",
        "        B (dict): tthe HMM emmission probabilities.\n",
        "\n",
        "        Parameters for an LSTM: the model dictionary (allows tagging the given sentence)\n",
        "\n",
        "\n",
        "    Return:\n",
        "        list: list of pairs\n",
        "    \"\"\"\n",
        "    if list(model.keys())[0]=='baseline':\n",
        "        return baseline_tag_sentence(sentence, list(model.values())[0][0], list(model.values())[0][1])\n",
        "    if list(model.keys())[0]=='hmm':\n",
        "        return hmm_tag_sentence(sentence, list(model.values())[0][0], list(model.values())[0][1])\n",
        "    if list(model.keys())[0] == 'blstm':\n",
        "        return rnn_tag_sentence(sentence, list(model.values())[0])\n",
        "    if list(model.keys())[0] == 'cblstm':\n",
        "        return rnn_tag_sentence(sentence, list(model.values())[0])\n",
        "\n",
        "def count_correct(gold_sentence, pred_sentence):\n",
        "  \"\"\"Return the total number of correctly predicted tags,the total number of\n",
        "  correcttly predicted tags for oov words and the number of oov words in the\n",
        "  given sentence.\n",
        "\n",
        "  Args:\n",
        "      gold_sentence (list): list of pairs, assume to be gold labels\n",
        "      pred_sentence (list): list of pairs, tags are predicted by tagger\n",
        "\n",
        "  \"\"\"\n",
        "  assert len(gold_sentence)==len(pred_sentence)\n",
        "  print(\"gold_sentence\", gold_sentence)\n",
        "  print(\"pred_sentence\", pred_sentence)\n",
        "\n",
        "  vocab = perWordTagCounts.keys()\n",
        "  OOV = sum([0 if pair[0] in vocab else 1 for pair in gold_sentence])\n",
        "\n",
        "  correct = 0\n",
        "  correctOOV = 0\n",
        "\n",
        "  for i in range(len(gold_sentence)):\n",
        "    if gold_sentence[i][1] == pred_sentence[i][1]:\n",
        "      correct+=1\n",
        "      if gold_sentence[i][0] not in vocab:\n",
        "        correctOOV+=1\n",
        "    \n",
        "\n",
        "  return correct, correctOOV, OOV"
      ],
      "metadata": {
        "id": "fEadiW_zzgD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}